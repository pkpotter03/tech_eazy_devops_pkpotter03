name: Multi-Stage Deployment (Assignment 4)

on:
  push:
    branches: [ main ]
    tags:
      - 'deploy-*'     # e.g., deploy-dev, deploy-qa, deploy-prod
  workflow_dispatch:
    inputs:
      stage:
        description: "Stage (Dev/QA/Prod)"
        required: true
        default: "Dev"
        type: choice
        options:
          - Dev
          - QA
          - Prod

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}

jobs:
  provision:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: infra/terraform
    outputs:
      public_ip: ${{ steps.outputs_step.outputs.public_ip }}
      stage: ${{ steps.stage.outputs.stage }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Determine Stage
        id: stage
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            STAGE="${{ github.event.inputs.stage }}"
          elif [[ "${{ github.ref_name }}" == deploy-* ]]; then
            TAG="${{ github.ref_name }}"
            # expect deploy-dev, deploy-qa, or deploy-prod
            SUFFIX="${TAG#deploy-}"
            if [ "$SUFFIX" = "dev" ] || [ "$SUFFIX" = "Dev" ]; then STAGE="Dev"; fi
            if [ "$SUFFIX" = "qa" ] || [ "$SUFFIX" = "QA" ]; then STAGE="QA"; fi
            if [ "$SUFFIX" = "prod" ] || [ "$SUFFIX" = "Prod" ]; then STAGE="Prod"; fi
          else
            STAGE="Dev"
          fi
          echo "stage=$STAGE" >> $GITHUB_OUTPUT

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform init

      - name: Import existing resources (idempotent)
        run: |
          set -e
          STAGE="${{ steps.stage.outputs.stage }}"
          STAGE_LOWER=$(echo "$STAGE" | tr '[:upper:]' '[:lower:]')
          LOG_BUCKET_NAME="${{ secrets.TF_VAR_log_bucket_name }}"

          # Default VPC
          VPC_ID=$(aws ec2 describe-vpcs --filters Name=isDefault,Values=true --query "Vpcs[0].VpcId" --output text)

          # Security Group (assignment4-<stage>-sg)
          SG_ID=$(aws ec2 describe-security-groups \
            --filters Name=group-name,Values=assignment4-$STAGE_LOWER-sg Name=vpc-id,Values=$VPC_ID \
            --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || true)
          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            echo "Importing SG $SG_ID" && terraform import -input=false aws_security_group.app_sg "$SG_ID" || true
          fi

          # S3 Bucket
          if aws s3api head-bucket --bucket "$LOG_BUCKET_NAME" 2>/dev/null; then
            echo "Importing bucket $LOG_BUCKET_NAME" && terraform import -input=false aws_s3_bucket.logs "$LOG_BUCKET_NAME" || true
          fi

          # IAM Policy (S3WriteOnlyAccess-<stage>)
          POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='S3WriteOnlyAccess-$STAGE_LOWER'].Arn" --output text 2>/dev/null || true)
          if [ -n "$POLICY_ARN" ] && [ "$POLICY_ARN" != "None" ]; then
            echo "Importing policy $POLICY_ARN" && terraform import -input=false aws_iam_policy.s3_write_only "$POLICY_ARN" || true
          fi

          # IAM Roles
          if aws iam get-role --role-name "S3ReadOnlyRole-$STAGE_LOWER" >/dev/null 2>&1; then
            echo "Importing role S3ReadOnlyRole-$STAGE_LOWER" && terraform import -input=false aws_iam_role.s3_read_only_role "S3ReadOnlyRole-$STAGE_LOWER" || true
          fi
          if aws iam get-role --role-name "S3WriteOnlyRole-$STAGE_LOWER" >/dev/null 2>&1; then
            echo "Importing role S3WriteOnlyRole-$STAGE_LOWER" && terraform import -input=false aws_iam_role.s3_write_only_role "S3WriteOnlyRole-$STAGE_LOWER" || true
          fi

          # Instance Profile
          if aws iam get-instance-profile --instance-profile-name "S3WriteOnlyInstanceProfile-$STAGE_LOWER" >/dev/null 2>&1; then
            echo "Importing instance profile S3WriteOnlyInstanceProfile-$STAGE_LOWER" && terraform import -input=false aws_iam_instance_profile.write_only_profile "S3WriteOnlyInstanceProfile-$STAGE_LOWER" || true
          fi

      - name: Terraform Plan
        run: |
          # Determine stage-specific values
          STAGE="${{ steps.stage.outputs.stage }}"
          STAGE_LOWER=$(echo "$STAGE" | tr '[:upper:]' '[:lower:]')
          
          # Get stage-specific config values
          source ../../scripts/${STAGE}_config
          
          terraform plan \
            -var="region=${{ env.AWS_REGION }}" \
            -var="stage=$STAGE_LOWER" \
            -var="instance_type=$INSTANCE_TYPE" \
            -var="key_name=${{ secrets.TF_VAR_key_name }}" \
            -var="log_bucket_name=${{ secrets.TF_VAR_log_bucket_name }}" \
            -var="app_port=$APP_PORT" \
            -var="github_repo_type=$REPO_TYPE"

      - name: Terraform Apply
        run: |
          # Determine stage-specific values
          STAGE="${{ steps.stage.outputs.stage }}"
          STAGE_LOWER=$(echo "$STAGE" | tr '[:upper:]' '[:lower:]')
          
          # Get stage-specific config values
          source ../../scripts/${STAGE}_config
          
          terraform apply -auto-approve \
            -var="region=${{ env.AWS_REGION }}" \
            -var="stage=$STAGE_LOWER" \
            -var="instance_type=$INSTANCE_TYPE" \
            -var="key_name=${{ secrets.TF_VAR_key_name }}" \
            -var="log_bucket_name=${{ secrets.TF_VAR_log_bucket_name }}" \
            -var="app_port=$APP_PORT" \
            -var="github_repo_type=$REPO_TYPE"

      - name: Collect Outputs
        id: outputs_step
        run: |
          echo "public_ip=$(terraform output -raw public_ip)" >> $GITHUB_OUTPUT

  deploy:
    runs-on: ubuntu-latest
    needs: provision
    env:
      PUBLIC_IP: ${{ needs.provision.outputs.public_ip }}
      STAGE: ${{ needs.provision.outputs.stage }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set Stage value
        id: stage
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            STAGE="${{ github.event.inputs.stage }}"
          elif [[ "${{ github.ref_name }}" == deploy-* ]]; then
            TAG="${{ github.ref_name }}"
            SUFFIX="${TAG#deploy-}"
            if [ "$SUFFIX" = "dev" ] || [ "$SUFFIX" = "Dev" ]; then STAGE="Dev"; fi
            if [ "$SUFFIX" = "qa" ] || [ "$SUFFIX" = "QA" ]; then STAGE="QA"; fi
            if [ "$SUFFIX" = "prod" ] || [ "$SUFFIX" = "Prod" ]; then STAGE="Prod"; fi
          else
            STAGE="Dev"
          fi
          echo "stage=$STAGE" >> $GITHUB_OUTPUT

      - name: Write SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H $PUBLIC_IP >> ~/.ssh/known_hosts

      - name: Upload deploy artifacts (script, configs, .env)
        run: |
          # Prepare stage-specific config filename expected by your script
          STAGE="${{ steps.stage.outputs.stage }}"
          STAGE_LOWER=$(echo "$STAGE" | tr '[:upper:]' '[:lower:]')
          
          # Copy stage-specific configs
          cp scripts/${STAGE}_config /tmp/${STAGE}_config
          cp scripts/${STAGE_LOWER}.json /tmp/app-config.json
          
          # Copy .env template for Assignment 2 parts
          cp scripts/env.template /tmp/.env

          scp -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no \
            scripts/deploy.sh ubuntu@$PUBLIC_IP:/home/ubuntu/deploy.sh

          scp -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no \
            /tmp/${STAGE}_config ubuntu@$PUBLIC_IP:/home/ubuntu/${STAGE}_config

          scp -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no \
            /tmp/app-config.json ubuntu@$PUBLIC_IP:/home/ubuntu/app-config.json

          scp -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no \
            /tmp/.env ubuntu@$PUBLIC_IP:/home/ubuntu/.env

      - name: Run deploy.sh remotely
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$PUBLIC_IP \
            "export GITHUB_TOKEN='$GITHUB_TOKEN' && chmod +x /home/ubuntu/deploy.sh && cd /home/ubuntu && sudo -E ./deploy.sh ${{ steps.stage.outputs.stage }}"

  health_check:
    runs-on: ubuntu-latest
    needs: [provision, deploy]
    env:
      PUBLIC_IP: ${{ needs.provision.outputs.public_ip }}
      STAGE: ${{ needs.provision.outputs.stage }}
    steps:
      - name: Poll application endpoint
        run: |
          set -e
          STAGE_LOWER=$(echo "$STAGE" | tr '[:upper:]' '[:lower:]')
          
          # Get app port from stage config
          source scripts/${STAGE}_config
          
          URL="http://$PUBLIC_IP:$APP_PORT/hello"
          echo "Polling $URL for stage $STAGE (port $APP_PORT) for up to 3 minutes..."
          
          for i in {1..36}; do
            if curl -s --max-time 5 "$URL" | grep -q "Hello from Spring MVC!"; then
              echo "✅ App is healthy on port $APP_PORT for stage $STAGE"
              exit 0
            fi
            sleep 5
          done
          echo "❌ Health check failed for stage $STAGE on port $APP_PORT"
          exit 1

      - name: If failed, fetch remote logs (best-effort)
        if: failure()
        run: |
          mkdir -p logs
          echo "Attempting to SSH and fetch app.log for stage $STAGE"
          set +e
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$PUBLIC_IP "sudo tail -n 200 /home/ubuntu/app.log" || true
          echo "You can also check S3 logs in bucket: ${{ secrets.TF_VAR_log_bucket_name }}/logs/$STAGE_LOWER/"
